{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# warc_reader.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes a WARC file/collection of WARC files and a list of URLs supplied via .txt file.\n",
    "\n",
    "The notebook reads the WARC files and determines which URLs are present/missing as specified by the URL list.\n",
    "The notebook also reads the HTML content of the URLs specified to find problematic elements in the WARC files.\n",
    "\n",
    "It is currently configured to find the following elements for a recurring web crawl -\n",
    " - &lt;title>Error response&lt;/title>\n",
    " - &lt;section class=\"securing-warning\"> and &lt;div id=\"restricted\">\n",
    " - &lt;div class=\"more-link\">\n",
    " - &lt;div class=\"tab-placeholder\">\n",
    " - &lt;div class=\"c-filter--dynamic\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "!{sys.executable} -m pip install warcio # Install warcio into conda environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_warc_paths(warc_path):\n",
    "    \"\"\"Get WARC file paths from file/directory path, filtering out non-WARC files\"\"\"\n",
    "    warc_paths = []\n",
    "    if os.path.isfile(warc_path):\n",
    "        if Path(warc_path).match('*warc*'):\n",
    "            warc_paths.append(warc_path)\n",
    "    elif os.path.isdir(warc_path):\n",
    "        warc_files = os.listdir(warc_path)\n",
    "        for filename in warc_files:\n",
    "            warc_path_tmp = os.path.join(warc_path, filename)\n",
    "            if not Path(warc_path_tmp).match('*warc*'):\n",
    "                continue\n",
    "            warc_paths.append(warc_path_tmp)\n",
    "    return warc_paths\n",
    "\n",
    "\n",
    "def element_test(soup, tag, attr_type, attr_val):\n",
    "    \"\"\"Return True or False if HTML contains element as defined in soup.findAll\"\"\"\n",
    "    out = soup.findAll(tag, attrs={attr_type: attr_val})\n",
    "    return bool(out)\n",
    "\n",
    "\n",
    "def read_file(url_list):\n",
    "    \"\"\"Load the .txt file and return list\"\"\"\n",
    "    with open(url_list, 'r') as f:\n",
    "        lines_file = [line.strip() for line in f]\n",
    "    return lines_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get WARC filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARC_PATHS = get_warc_paths(input('Path to WARC file/directory: '))\n",
    "print(f'Number of WARC files: {len(WARC_PATHS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get URL list from .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS = read_file(input('Path to URL .txt list: '))\n",
    "print(f'Number of URLs in list: {len(URLS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ_RECORD_FROM_DATE = '20200101'\n",
    "READ_RECORD_TO_DATE = '20221120'\n",
    "\n",
    "# Create datetime objects\n",
    "read_from = datetime.strptime(READ_RECORD_FROM_DATE, \"%Y%m%d\")\n",
    "read_to = datetime.strptime(READ_RECORD_TO_DATE, \"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read WARC files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active members\n",
    "from collections import defaultdict\n",
    "dates_dict = defaultdict(set)\n",
    "\n",
    "WARC_Target_URI_list = []  # List contains ALL URIs found in WARC files\n",
    "\n",
    "# HTML elements testing\n",
    "error_response = []\n",
    "restricted_page = []\n",
    "more_link = []\n",
    "tab_placeholder = []\n",
    "c_filter_dynamic = []\n",
    "\n",
    "for warc in tqdm(WARC_PATHS):\n",
    "    with open(warc, 'rb') as stream:\n",
    "        for record in ArchiveIterator(stream):\n",
    "            # Create date_object for each record\n",
    "            try:\n",
    "                date_object = datetime.strptime(\n",
    "                    record.rec_headers.get_header('WARC-Date'), \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            except:\n",
    "                try:\n",
    "                    date_object = datetime.strptime(\n",
    "                        record.rec_headers.get_header('WARC-Date'), \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "            # Compare date_object with READ_FROM_DATE and READ_TO_DATE parameters\n",
    "            if read_from < date_object and read_to > date_object:\n",
    "                # Append URI to WARC_Target_URI_list - this creates a complete list of URIs in the WARC files\n",
    "                WARC_Target_URI_list.append(\n",
    "                    str(record.rec_headers.get_header('WARC-Target-URI')))\n",
    "                \n",
    "                # Active members\n",
    "                if 'membership/active-members' in str(record.rec_headers.get_header('WARC-Target-URI')):\n",
    "                    print(f\"{record.rec_headers.get_header('WARC-Target-URI')} found in {warc}\")\n",
    "                    dates_dict[warc].add(record.rec_headers.get_header('WARC-Target-URI'))\n",
    "\n",
    "                if record.rec_type == 'response':\n",
    "                    try:\n",
    "                        if 'text/html' in str(record.http_headers.get_header('Content-Type')):\n",
    "                            # Loop through URLS, if a URL matches the record - read HTML and look for elements\n",
    "                            for URL in URLS:\n",
    "                                if URL in str(record.rec_headers.get_header('WARC-Target-URI')):\n",
    "                                    # Get HTML\n",
    "                                    # Decode bytes to utf-8 string and strip whitespace\n",
    "                                    html = record.content_stream().read().decode('utf-8').strip()\n",
    "                                    # Create soup object\n",
    "                                    soup = BeautifulSoup(html, \"html.parser\")\n",
    "                                    \n",
    "                                    # Build list of pages which have 'Error response' in title\n",
    "                                    if soup.find(\"title\"):\n",
    "                                        if soup.find(\"title\").string == 'Error response':\n",
    "                                            error_response.append(record.rec_headers.get_header('WARC-Target-URI'))\n",
    "                                    \n",
    "                                    # Build list of Boolean values - Restricted content\n",
    "                                    if element_test(soup, tag=\"section\", attr_type=\"class\", attr_val=\"secure-warning\") or element_test(soup, tag=\"div\", attr_type=\"id\", attr_val=\"restricted\"):\n",
    "                                        restricted_page.append(\n",
    "                                            record.rec_headers.get_header('WARC-Target-URI'))\n",
    "\n",
    "                                    # Build list of Boolean values - More-link\n",
    "                                    if element_test(soup, tag=\"div\", attr_type=\"class\", attr_val=\"more-link\"):\n",
    "                                        more_link.append(record.rec_headers.get_header('WARC-Target-URI'))\n",
    "\n",
    "                                    # Build list of Boolean values - Tab-placeholder\n",
    "                                    if element_test(soup, tag=\"div\", attr_type=\"class\", attr_val=\"tab-placeholder\"):\n",
    "                                        tab_placeholder.append(\n",
    "                                            record.rec_headers.get_header('WARC-Target-URI'))\n",
    "\n",
    "                                    # Build list of Boolean values - C-filter--dynamic\n",
    "                                    if element_test(soup, tag=\"div\", attr_type=\"class\", attr_val=\"c-filter--dynamic\"):\n",
    "                                        c_filter_dynamic.append(record.rec_headers.get_header('WARC-Target-URI'))\n",
    "                                        \n",
    "                    except Exception as e:\n",
    "                        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# # pretty print dict as json\n",
    "# print(json.dumps(dates_dict, indent=2))\n",
    "# print(dates_dict)\n",
    "\n",
    "for k, v in dates_dict.items():\n",
    "    print(k)\n",
    "    for v in dates_dict[k]:\n",
    "        print('\\t' + v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Found pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_in_URLS = []\n",
    "for i in list(set(WARC_Target_URI_list)):\n",
    "    if i in URLS:\n",
    "        found_in_URLS.append(i)\n",
    "print(f'Found (from URL list provided): {len(found_in_URLS)}/{len(URLS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_in_URLS = sorted(list(set(URLS) - set(found_in_URLS)))\n",
    "print(f'Missing (from URL list provided): {len(missing_in_URLS)}/{len(URLS)}')\n",
    "for i in missing_in_URLS:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error response pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_response_in_URLS = []\n",
    "for i in sorted(list(set(error_response))): # set -> list removes duplicates\n",
    "    if i in URLS: # only interested if the 'Error response' in title is found in URL list that is being tested\n",
    "        error_response_in_URLS.append(i)\n",
    "print(f'Found \\'Error response\\' pages: {len(error_response_in_URLS)}')\n",
    "for i in error_response_in_URLS:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restricted pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_in_URLS = []\n",
    "for i in sorted(list(set(restricted_page))): # set -> list removes duplicates\n",
    "    if i in URLS: # only interested if the restricted page is found in URL list that is being tested\n",
    "        restricted_in_URLS.append(i)\n",
    "print(f'Found restricted pages (pages with <section class=\"secure-warning\"> or <div id=\"restricted\">): {len(restricted_in_URLS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read WARC files again - checking for logged-in versions of found restricted pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loops through WARC files again, testing against the restricted_in_URLS list; looking for a version of the page that does\n",
    "# NOT contain <section class=\"secure-warning\"> or <div id=\"restricted\">.\n",
    "restricted_page_logged_in = []\n",
    "\n",
    "for warc in tqdm(WARC_PATHS):\n",
    "    with open(warc, 'rb') as stream:\n",
    "        for record in ArchiveIterator(stream):\n",
    "            # Create date_object for each record\n",
    "            try:\n",
    "                date_object = datetime.strptime(\n",
    "                    record.rec_headers.get_header('WARC-Date'), \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            except:\n",
    "                try:\n",
    "                    date_object = datetime.strptime(\n",
    "                        record.rec_headers.get_header('WARC-Date'), \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            # Compare date_object with READ_FROM_DATE and READ_TO_DATE parameters\n",
    "            if read_from < date_object and read_to > date_object:\n",
    "                if record.rec_type == 'response':\n",
    "                    try:\n",
    "                        if 'text/html' in str(record.http_headers.get_header('Content-Type')):\n",
    "                            for page in restricted_in_URLS:\n",
    "                                if page in str(record.rec_headers.get_header('WARC-Target-URI')):\n",
    "                                    # Decode bytes to utf-8 string and strip whitespace\n",
    "                                    html = record.content_stream().read().decode('utf-8').strip()\n",
    "                                    # Create soup object\n",
    "                                    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "                                    # Build list of Boolean values - Restricted content - the not negates the True\n",
    "                                    if not element_test(soup, tag=\"section\", attr_type=\"class\", attr_val=\"secure-warning\") and not element_test(soup, tag=\"div\", attr_type=\"id\", attr_val=\"restricted\"):\n",
    "                                        restricted_page_logged_in.append(\n",
    "                                            record.rec_headers.get_header('WARC-Target-URI'))\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of URLs where a restricted page and logged-in page exist in the WARC files\n",
    "# for i in list(set(restricted_page_logged_in)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds any example where a page with a restricted banner was found, but a logged-in counterpart was not\n",
    "missing_logged_in = sorted(list(set(restricted_in_URLS) - set(restricted_page_logged_in)))\n",
    "print(f'Restricted pages without a logged-in counterpart: {len(missing_logged_in)}')\n",
    "for i in missing_logged_in:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_link_in_URLS = []\n",
    "for i in sorted(list(set(more_link))): # set -> list removes duplicates\n",
    "    if i in URLS: # only interested if the more_link element is found in URL list that is being tested\n",
    "        more_link_in_URLS.append(i)\n",
    "print(f'Found more-link pages (pages with <div class=\"more-link\">): {len(more_link_in_URLS)}')\n",
    "for i in more_link_in_URLS:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tab_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_placeholder_in_URLS = []\n",
    "for i in sorted(list(set(tab_placeholder))): # set -> list removes duplicates\n",
    "    if i in URLS: # only interested if the tab_placeholder element is found in URL list that is being tested\n",
    "        tab_placeholder_in_URLS.append(i)\n",
    "print(f'Found tab_placeholder pages (pages with <div class=\"tab_placeholder\">): {len(tab_placeholder_in_URLS)}')\n",
    "for i in tab_placeholder_in_URLS:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c_filter_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_filter_dynamic_in_URLS = []\n",
    "for i in sorted(list(set(c_filter_dynamic))): # set -> list removes duplicates\n",
    "    if i in URLS: # only interested if the c_filter_dynamic element is found in URL list that is being tested\n",
    "        c_filter_dynamic_in_URLS.append(i)\n",
    "print(f'Found c_filter_dynamic pages (pages with <div class=\"c-filter--dynamic\">): {len(c_filter_dynamic_in_URLS)}')\n",
    "for i in c_filter_dynamic_in_URLS:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "69a4ae0f6de19cbe3fe0b70958d333073c24c6c7431c5775c02db42713ea0cac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
